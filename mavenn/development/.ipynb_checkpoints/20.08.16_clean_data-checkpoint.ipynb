{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import suftware as su\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson\n",
    "\n",
    "e = np.exp(1)\n",
    "pi = np.pi\n",
    "def pseudo_log(x,base=e):\n",
    "    return np.log(x+.5)/np.log(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_activities(bc_df, bootstrap=False, seed=None):\n",
    "        \"\"\"\n",
    "        This function computes log_psi for all splice sites\n",
    "        \"\"\"\n",
    "        \n",
    "        # Seed random number generator\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy bc_df so original isn't changed\n",
    "        bc_df = bc_df.copy()\n",
    "        \n",
    "        # Add bc_ct col\n",
    "        bc_df['bc_ct'] = 1\n",
    "        \n",
    "        # Compute weights for bootstrapping\n",
    "        num_bcs = len(bc_df)\n",
    "        if bootstrap:\n",
    "            weights = poisson.rvs(mu=1.0, size=num_bcs)\n",
    "        else:\n",
    "            weights = np.ones(num_bcs)\n",
    "            \n",
    "        # Multipy ct cols through by weights\n",
    "        ct_cols = [col for col in bc_df.columns if '_ct' in col]\n",
    "        for col in ct_cols:\n",
    "            bc_df[col] = bc_df[col]*weights\n",
    "            \n",
    "        # Marginalize by splice site\n",
    "        ss_df = bc_df.groupby('ss').sum()\n",
    "        \n",
    "        # Divide by total number of counts\n",
    "        ss_df = ss_df\n",
    "        \n",
    "        # Store logs centered on medians\n",
    "        ss_df['log_psi'] = pseudo_log(ss_df['ex_ct']) - pseudo_log(ss_df['tot_ct'])\n",
    "\n",
    "        # Remove unecessary columns\n",
    "        cols_to_keep = ['log_psi']\n",
    "        return ss_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_data(in_file, \n",
    "                 min_ct_per_bc=1, \n",
    "                 min_num_barcodes=10, \n",
    "                 num_resamps=100, \n",
    "                 min_sigma=1E-2, \n",
    "                 report_every=10,\n",
    "                 estimate_mi=True):\n",
    "        \n",
    "    # Create dict to record statistics\n",
    "    stats_dict = {}\n",
    "        \n",
    "    # Load file\n",
    "    in_df = pd.read_csv(in_file, delimiter='\\t', index_col=0)\n",
    "    print(f'Processing {in_file}')\n",
    "\n",
    "    # Remove extraneous columns\n",
    "    del in_df['mis_ct']\n",
    "    del in_df['lib_ct']\n",
    "\n",
    "    # Get total number of barcodes\n",
    "    num_bcs_total = len(in_df)\n",
    "\n",
    "    # Only keep barcodes that have a minimum number of counts in all bc samples\n",
    "    ix = (in_df['tot_ct'] + in_df['ex_ct'] >=min_ct_per_bc)\n",
    "    bc_df = in_df[ix]\n",
    "    \n",
    "    # Compute the number of splice sites per bc\n",
    "    ss_per_bc = bc_df[['ss','bc']].groupby('ss').count()\n",
    "\n",
    "    # Only keep splice sites that have at least 10 barcodes\n",
    "    ix = (ss_per_bc['bc']>=min_num_barcodes)\n",
    "    ss_to_keep = ss_per_bc[ix].index\n",
    "\n",
    "    # Remove barcodes linked to unusued splice sites\n",
    "    ix = bc_df['ss'].isin(ss_to_keep)\n",
    "    bc_df = bc_df[ix]\n",
    "    \n",
    "    # provide feedback\n",
    "    stats_dict['num_ss'] = len(ss_to_keep)\n",
    "    stats_dict['pct_ss'] = 100*len(ss_to_keep)/len(ss_per_bc)\n",
    "    stats_dict['num_bc'] = len(bc_df)\n",
    "    stats_dict['pct_bc'] = 100*len(bc_df)/len(in_df)\n",
    "    stats_dict['sum_tot_ct'] = bc_df[\"tot_ct\"].sum()\n",
    "    stats_dict['sum_ex_ct'] = bc_df[\"ex_ct\"].sum()\n",
    "    \n",
    "    # Get best estimate of activites\n",
    "    ss_df = compute_activities(bc_df, bootstrap=False)\n",
    "    \n",
    "    # Get boostrap resampled estimates\n",
    "    print(f'Doing bootstrap reampling:',end='')\n",
    "    resampled_dfs = []\n",
    "    for n in range(num_resamps):\n",
    "        if n%report_every==0 and n>0:\n",
    "            print('.', end='')\n",
    "        ss_resamp_df = compute_activities(bc_df, bootstrap=True, seed=n)\n",
    "        resampled_dfs.append(ss_resamp_df)\n",
    "    print('')\n",
    "        \n",
    "    # Compute std for each column in ss_df\n",
    "    for col in ss_df.columns:\n",
    "        std_col = 'd'+col\n",
    "        vals = np.array([df[col].values for df in resampled_dfs]).T\n",
    "        ss_df[std_col] = vals.std(axis=1, ddof=1)\n",
    "\n",
    "    # Get number of splice sites\n",
    "    num_ss = len(ss_df)\n",
    "        \n",
    "    if estimate_mi:\n",
    "        # Compute conditional entropy\n",
    "        sigma = ss_df['dlog_psi'].values\n",
    "        sigma[sigma<min_sigma]=min_sigma\n",
    "        H_contributions = 0.5*np.log2(2*e*pi*sigma**2)\n",
    "        H_ygx = np.mean(H_contributions)\n",
    "        dH_ygx = np.std(H_contributions, ddof=1)/np.sqrt(num_ss)\n",
    "        #stats_dict['H[y|yhat]'] = H_ygyhat\n",
    "        #stats_dict['dH[y|yhat]'] = dH_ygyhat\n",
    "\n",
    "        # Compute entropy \n",
    "        y = ss_df['log_psi'].values\n",
    "        p_y = su.DensityEstimator(y)\n",
    "        H_y = -p_y.get_stats().loc['posterior mean','entropy']\n",
    "        dH_y = p_y.get_stats().loc['posterior RMSD','entropy']\n",
    "        #stats_dict['H[y]'] = H_y\n",
    "        #stats_dict['dH[y]'] = dH_y\n",
    "\n",
    "        # Report final mutual information value\n",
    "        I_y_x = H_y - H_ygx\n",
    "        dI_y_x = np.sqrt(dH_y**2 + dH_ygx**2)\n",
    "        stats_dict['I[y;x]'] = I_y_x\n",
    "        stats_dict['dI[y;x]'] = dI_y_x\n",
    "        \n",
    "    return ss_df, stats_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts cleans the `results` files from Wong et al. 2018. \n",
    "1. Barcodes with no reads in tot_ct or ex_ct are removed.\n",
    "2. Splice sites with fewer than 10 associated barcodes are removed.\n",
    "3. The best estimate for log_psi for each splice site is computed as the ratio ex_ct/tot_ct, where each of these quantities is summed across all barcodes, using a pseudocount of 0.5.\n",
    "4. The standard error of these estimates is comptued by bootstrap resampling all barcodes, then re-computing these ratios for each splice site. Note that this is NOT ideal; ideally we would bootstrap resample for each splice site individually. \n",
    "5. Sample statistics, including intrinsic information, is then recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 21 files to process.\n",
      "Processing /Users/jkinney/Dropbox/15_mpathic/20_mpathic_redo/20.08.16_mpsa_raw_data/results.brca2_9nt_lib1_rep1.txt\n",
      "Doing bootstrap reampling:.........\n",
      "Output written to 20.08.16_mpsa_data/brca2_lib1_rep1.csv.\n",
      "\n",
      "Processing /Users/jkinney/Dropbox/15_mpathic/20_mpathic_redo/20.08.16_mpsa_raw_data/results.brca2_9nt_lib1_rep2.txt\n",
      "Doing bootstrap reampling:........."
     ]
    }
   ],
   "source": [
    "in_dir = '/Users/jkinney/Dropbox/15_mpathic/20_mpathic_redo/20.08.16_mpsa_raw_data'\n",
    "out_dir = '20.08.16_mpsa_data'\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"error\")\n",
    "\n",
    "# Clear output directory\n",
    "files = glob.glob(f'{out_dir}/*.*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Get list of input files and sort them\n",
    "in_files = glob.glob(f'{in_dir}/results.*.txt')\n",
    "in_files.sort()\n",
    "print(f'-> {len(in_files)} files to process.')\n",
    "\n",
    "stats_df = pd.DataFrame()\n",
    "\n",
    "# For each input file\n",
    "for in_file in in_files:\n",
    "    \n",
    "    # Determine name of sample\n",
    "    m = re.match(\".*/results.(?P<locus>[^_]+)_9nt_(?P<librep>.+).txt\",in_file)\n",
    "    locus = m.groupdict()['locus']\n",
    "    librep = m.groupdict()['librep']\n",
    "    name = f'{locus}_{librep}'\n",
    "    \n",
    "    # Process sample\n",
    "    ss_df, stats_dict = process_data(in_file, num_resamps=100)\n",
    "    stats_dict['name'] = name\n",
    "    stats_df = stats_df.append(stats_dict, ignore_index=True)\n",
    "    \n",
    "    # Save cleaned data from sample\n",
    "    out_file = f'{out_dir}/{name}.csv'\n",
    "    ss_df.to_csv(out_file)\n",
    "    print(f'Output written to {out_file}.\\n')\n",
    "    \n",
    "# Save dataframe containing sample stats\n",
    "stats_df.set_index('name', inplace=True, drop=True)\n",
    "stats_df = stats_df[['num_bc','pct_bc','num_ss','pct_ss','sum_tot_ct','sum_ex_ct','I[y;x]','dI[y;x]']]\n",
    "stats_df.to_csv(f'{out_dir}/stats.csv')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
