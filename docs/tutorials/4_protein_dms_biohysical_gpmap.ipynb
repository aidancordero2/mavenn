{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Protein DMS modeling using a biophysical G-P map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Special imports\n",
    "import mavenn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to train and visualize a thermodynamic model for IgG binding by GB1. This model is trained on the built-in `'gb1'` dataset provided with MAVE-NN. A similar (though not identical) model is featured in Fig. 6b of Tareen et al.\n",
    "(2021).\n",
    "\n",
    "## Training\n",
    "\n",
    "First we load the `'gb1'` dataset, compute the length of sequences in this dataset, and split these data into a training+validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'gb1' \n",
      "Sequence length: 55 amino acids\n",
      "Training set   :  318,560 observations (  60.02%)\n",
      "Validation set :  106,115 observations (  19.99%)\n",
      "Test set       :  106,062 observations (  19.98%)\n",
      "-------------------------------------------------\n",
      "Total dataset  :  530,737 observations ( 100.00%)\n",
      "\n",
      "Train + val set size : 424,675 observations\n",
      "Test set size        : 106,062 observations\n",
      "trainval_df:\n"
     ]
    },
    {
     "data": {
      "text/plain": "        validation  dist  input_ct  selected_ct         y  \\\n0            False     2       173           33 -3.145154   \n1             True     2        18            8 -1.867676   \n2             True     2        66            2 -5.270800   \n3            False     2        72            1 -5.979498   \n4            False     2        69          168  0.481923   \n...            ...   ...       ...          ...       ...   \n424670        True     2       151           30 -3.083405   \n424671       False     2       462          139 -2.515259   \n424672       False     2       317           84 -2.693165   \n424673       False     2       335           77 -2.896589   \n424674       False     2        95           16 -3.287173   \n\n                                                        x  \n0       AAKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n1       ACKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n2       ADKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n3       AEKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n4       AFKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n...                                                   ...  \n424670  QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n424671  QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n424672  QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n424673  QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n424674  QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...  \n\n[424675 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>validation</th>\n      <th>dist</th>\n      <th>input_ct</th>\n      <th>selected_ct</th>\n      <th>y</th>\n      <th>x</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>2</td>\n      <td>173</td>\n      <td>33</td>\n      <td>-3.145154</td>\n      <td>AAKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>2</td>\n      <td>18</td>\n      <td>8</td>\n      <td>-1.867676</td>\n      <td>ACKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>True</td>\n      <td>2</td>\n      <td>66</td>\n      <td>2</td>\n      <td>-5.270800</td>\n      <td>ADKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>2</td>\n      <td>72</td>\n      <td>1</td>\n      <td>-5.979498</td>\n      <td>AEKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>2</td>\n      <td>69</td>\n      <td>168</td>\n      <td>0.481923</td>\n      <td>AFKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>424670</th>\n      <td>True</td>\n      <td>2</td>\n      <td>151</td>\n      <td>30</td>\n      <td>-3.083405</td>\n      <td>QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>424671</th>\n      <td>False</td>\n      <td>2</td>\n      <td>462</td>\n      <td>139</td>\n      <td>-2.515259</td>\n      <td>QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>424672</th>\n      <td>False</td>\n      <td>2</td>\n      <td>317</td>\n      <td>84</td>\n      <td>-2.693165</td>\n      <td>QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>424673</th>\n      <td>False</td>\n      <td>2</td>\n      <td>335</td>\n      <td>77</td>\n      <td>-2.896589</td>\n      <td>QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n    <tr>\n      <th>424674</th>\n      <td>False</td>\n      <td>2</td>\n      <td>95</td>\n      <td>16</td>\n      <td>-3.287173</td>\n      <td>QYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDD...</td>\n    </tr>\n  </tbody>\n</table>\n<p>424675 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "data_name = 'gb1'\n",
    "print(f\"Loading dataset '{data_name}' \")\n",
    "\n",
    "# Load datset\n",
    "data_df = mavenn.load_example_dataset(data_name)\n",
    "\n",
    "# Get and report sequence length\n",
    "L = len(data_df.loc[0,'x'])\n",
    "print(f'Sequence length: {L:d} amino acids')\n",
    "\n",
    "# Split dataset\n",
    "trainval_df, test_df = mavenn.split_dataset(data_df)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f'Train + val set size : {len(trainval_df):6,d} observations')\n",
    "print(f'Test set size        : {len(test_df):6,d} observations')\n",
    "\n",
    "# Preview trainval_df\n",
    "print('trainval_df:')\n",
    "trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Thermodynamic Model\n",
    "The goal of the biophysical model for IgG binding by GB1 is to estimate the GB1 folding energy (relative to unfolded state energy), $\\Delta G_F$, and IgG-GB1 binding energy, $\\Delta G_B$.\n",
    "Using quasi-equilibrium assumption, one can define the three-microsstate model summerized in the table below: \n",
    "\n",
    "|    microstates   |    Gibbs free energies   |\n",
    "|:----------------:|:------------------------:|\n",
    "|     unfolded     |             0            |\n",
    "| folded-unbound   |       $\\Delta G_F$       |\n",
    "|  folded-bound    | $\\Delta G_F +\\Delta G_B$ |\n",
    "\n",
    "To fit thermodynamic models, one can use the special **Custom G-P maps** layer implemented in `'mavenn.src.layers.gpmap'`. The implementation of this layer is similar to using custom layers in native `tensorflow` framework and consists of the following steps:\n",
    "\n",
    "1. Import some of built-in `tensorflow` functionalities and abstractions in our script. \n",
    "2. Define the super-class thermodynamic layer of the type `CustomGPMapLayer`: here it is called `class ThermodynamicLayer(CustomGPMapLayer)`.\n",
    "    - in the initialization step (`__init__` function) we can specify the overal parameters such as sequence length `L`, sequence alphabet `C` and neural-network weights regularizations `tf.keras.regularizers.L2`.\n",
    "\n",
    "3. Next, we need to build our layer. The Gibs free energies can be defined as a linear function of the sequence space with the form $G = \\mu_{i} + \\vec{\\theta_i} X$.\n",
    "    - $\\mu$ is the chemical potential\n",
    "    - $x$ is the one-hot encoded representation of the sequences.\n",
    "    - $i={f, b}$ representing folded and bounded microstates, respectively.\n",
    "    - Set of trainable G-P map parameters is $\\{\\mu_f, \\vec{\\theta_f}, \\mu_B, \\vec{\\theta_b} \\}$ which are the weights in our neural-network setup. \n",
    "4. The output of the layer should be our measurement, rate of the transcription `p_fb`, which has the notion of the probability of folded-bound state:\n",
    "\n",
    "$\n",
    "p_{fb} = \\frac{\\exp(-G_f-G_b)}{1+\\exp(-G_f)+ \\exp(-G_f-G_b)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mavenn.src.layers.gpmap import CustomGPMapLayer\n",
    "\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "class ThermodynamicLayer(CustomGPMapLayer):\n",
    "    \"\"\"\n",
    "    Represents a three stage thermodynamic model\n",
    "    containing the states:\n",
    "    1. unfolded \n",
    "    2. folded-unbound\n",
    "    3. folded-bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 L,\n",
    "                 C,\n",
    "                 regularizer, \n",
    "                 *args, **kwargs):\n",
    "        \"\"\"Construct layer instance.\"\"\"\n",
    "        \n",
    "        # set attributes\n",
    "        self.L = L # sequence length.\n",
    "        self.C= C  # sequence alphabets\n",
    "        self.regularizer = tf.keras.regularizers.L2(regularizer)\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build layer.\"\"\"\n",
    "        \n",
    "        # define bias/chemical potential weight for folding energy\n",
    "        self.theta_f_0 = self.add_weight(name='theta_f_0',\n",
    "                                         shape=(1,),\n",
    "                                         initializer=Constant(1.),\n",
    "                                         trainable=True,\n",
    "                                         regularizer=self.regularizer)\n",
    "\n",
    "        # define bias/chemical potential weight for binding energy\n",
    "        self.theta_b_0 = self.add_weight(name='theta_b_0',\n",
    "                                         shape=(1,),\n",
    "                                         initializer=Constant(1.),\n",
    "                                         trainable=True,\n",
    "                                         regularizer=self.regularizer)\n",
    "\n",
    "\n",
    "        # Define theta_binding_lc parameters\n",
    "        theta_b_lc_shape = (1, self.L, self.C)\n",
    "\n",
    "        theta_b_lc_init = np.random.randn(*theta_b_lc_shape)/np.sqrt(self.L)\n",
    "        self.theta_b_lc = self.add_weight(name='theta_b_lc',\n",
    "                                        shape=theta_b_lc_shape,\n",
    "                                        initializer=Constant(theta_b_lc_init),\n",
    "                                        trainable=True,\n",
    "                                        regularizer=self.regularizer)\n",
    "        \n",
    "        # Define theta_folding_lc parameters\n",
    "        theta_f_lc_shape = (1, self.L, self.C)\n",
    "\n",
    "        theta_f_lc_init = np.random.randn(*theta_f_lc_shape)/np.sqrt(self.L)\n",
    "        self.theta_f_lc = self.add_weight(name='theta_f_lc',\n",
    "                                          shape=theta_f_lc_shape,\n",
    "                                          initializer=Constant(theta_f_lc_init),\n",
    "                                          trainable=True,\n",
    "                                          regularizer=self.regularizer)\n",
    "\n",
    "        \n",
    "        # Call superclass build\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x_lc):\n",
    "        \"\"\"Process layer input and return output.\n",
    "        \n",
    "        x_lc: (tensor)\n",
    "            Input tensor that represents one-hot encoded \n",
    "            sequence values. \n",
    "        \"\"\"\n",
    "        \n",
    "        # reshape sequence to samples x length x characters\n",
    "        x_lc = tf.reshape(x_lc, [-1, self.L, self.C])\n",
    "        \n",
    "        # compute delta G for binding    \n",
    "        G_b = self.theta_b_0 + \\\n",
    "              tf.reshape(K.sum(self.theta_b_lc * x_lc, axis=[1, 2]),\n",
    "                         shape=[-1, 1])\n",
    "            \n",
    "        # compute delta G for folding\n",
    "        G_f = self.theta_f_0 + \\\n",
    "              tf.reshape(K.sum(self.theta_f_lc * x_lc, axis=[1, 2]),\n",
    "                         shape=[-1, 1])            \n",
    "        \n",
    "        # compute p_fb\n",
    "        p_fb = (K.exp(-G_f-G_b))/(1+K.exp(-G_f)+K.exp(-G_f-G_b))\n",
    "        \n",
    "        # return rate of transcription\n",
    "        return p_fb\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        \"\"\"Set values of layer parameters.\"\"\"\n",
    "        assert False\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        \"\"\"Get values of layer parameters.\"\"\"\n",
    "        param_dict = {}\n",
    "        param_dict['theta_b_0'] = np.float(self.theta_b_0)\n",
    "        param_dict['theta_f_0'] = np.float(self.theta_f_0)\n",
    "        param_dict['theta_b_lc'] = np.squeeze(self.theta_b_lc)\n",
    "        param_dict['theta_f_lc'] = np.squeeze(self.theta_f_lc)\n",
    "        return param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Now we create an instance of the `mavenn.Model` class, called `model`, using the following keyword arguments:\n",
    "\n",
    "- `L=L` specifies sequence length.\n",
    "- `alphabet='protein'` specifies that the alphabet our sequences are built from consists of 20 characters representing the 20 amino acids.\n",
    "- `gpmap_type='custom'` specifies that we wish to infer an custom G-P map.\n",
    "- `regression_type='GE'` specifies that our model will have a global epistasis (GE) measurement process. We choose this because our MAVE measurements are continuous real numbers.\n",
    "- `ge_noise_model_type='SkewedT'` specifies the use of a skewed-t noise model in the GE measurement process. The `'SkewedT'` noise model can accommodate asymmetric noise and is thus more flexible than the default `'Gaussian'` noise model.\n",
    "- `ge_heteroskedasticity_order=2` specifies that the noise model parameters (the three parameters of the skewed-t distribution) are each modeled using quadratic functions of the predicted measurement $\\hat{y}$. This will allow our model of experimental noise to vary with signal intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the training data by calling `model.set_data()`. The keyword argument `'validation_flags'` is used to specify which subset of the data in `trainval_df` will be used for validation (as opposed to stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model by calling `model.fit()`. In doing so we specify a number of hyperparameters including the learning rate, the number of epochs, the batch size, whether to use early stopping, and the early stopping patience.  We also set `verbose=False` to limit the amount of user feedback.\n",
    "\n",
    "Choosing hyperparameters is somewhat of an art, and the particular values used here were found by trial and error. In general users will have to try a number of different values for these and possibly other hyperparameters in order to find ones that work well. We recommend that users choose these hyperparameters in order to maximize the final value for `val_I_var`, the variational information of the trained model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define custom gp_map parameters dictionary\n",
    "gpmap_kwargs = {'L':L,\n",
    "                'C':20, # \n",
    "                'regularizer':0.05}\n",
    "\n",
    "# Create model instace\n",
    "model = mavenn.Model(L=L, \n",
    "                     alphabet='protein', \n",
    "                     regression_type='GE', \n",
    "                     gpmap_type='custom',\n",
    "                     ge_noise_model_type='SkewedT',\n",
    "                     ge_heteroskedasticity_order=2,\n",
    "                     gpmap_kwargs=gpmap_kwargs,\n",
    "                     custom_gpmap=ThermodynamicLayer)\n",
    "\n",
    "# Set training data\n",
    "model.set_data(x=trainval_df['x'],\n",
    "               y=trainval_df['y'],\n",
    "               validation_flags=trainval_df['validation'])\n",
    "\n",
    "# Train model\n",
    "model.fit(learning_rate=.001,\n",
    "          epochs=10,\n",
    "          batch_size=1024,\n",
    "          early_stopping=True,\n",
    "          early_stopping_patience=25,\n",
    "          verbose=False,\n",
    "          callbacks=[TqdmCallback(verbose=0)]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of our final trained model, we compute two different metrics on test data: **variational information** and **predictive information**. Variational information quantifies the performance of the full latent phenotype model, whereas predictive information quantifies the performance of just the G-P map portion of the model. See Tareen et al. (2021) for an expanded discussion of these quantities.\n",
    "\n",
    "Note that MAVE-NN also estimates the standard errors for these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predictive information on test data\n",
    "I_pred, dI_pred = model.I_predictive(x=test_df['x'], y=test_df['y'])\n",
    "print(f'test_I_pred: {I_pred:.3f} +- {dI_pred:.3f} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the trained model we call `model.save()`. This records our model in **two separate files**: a pickle file that defines model architecture (extension `'.pickle'`), and an H5 file that records model parameters (extension `'.h5'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "model_name = f'{data_name}_thermodynamic'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We now discuss how to visualize the training history, performance, and parameters of a trained model. First we  then load our model using `mavenn.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Delete model if it is present in memory\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load model from file\n",
    "model = mavenn.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.history` dictionary contains values for multiple model performance metrics, evaluated on both the training data and the validation data, as a function of training epoch. `'loss'` and `'val_loss'` record loss values, while `'I_var'` and `'val_I_var'` record the variational information values. As described in Tareen et al. (2021), these metrics are closely related: variational information is an affine transformation of model likelihood, whereas loss is equal to likelihood plus regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show metrics recorded in model.history()\n",
    "model.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting `'I_var'` and `'val_I_var'` versus epoch can often provide insight into the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot I_var_train, the variational information on training data as a function of epoch\n",
    "ax.plot(model.history['I_var'],\n",
    "        label=r'I_var_train')\n",
    "\n",
    "# Plot I_var_val, the variational information on validation data as a function of epoch\n",
    "ax.plot(model.history['val_I_var'],\n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Show I_pred_test, the predictive information of the final model on test data\n",
    "ax.axhline(I_pred, color='C3', linestyle=':',\n",
    "           label=r'test_I_pred')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('bits')\n",
    "ax.set_title('Training history: variational information')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we observe a few things:\n",
    "\n",
    "- `'val_I_var'` quickly converges to its final value, suggesting that model training has likely gone to completion.\n",
    "- `'I_var'` is noticably higher than `'val_I_var'`, which is indicative of overfitting. But these two quantities largely remain monotonically related to one another, indicating that this overfitting is likely begnin.\n",
    "- `'test_I_var'` is quite close to `'test_I_pred'` indicating that the inferred measurement process does a good job of describing the scatter of $y$ about $\\hat{y}$.\n",
    "\n",
    "Users can plot '`loss`' and '`var_loss`' if they like, though the absolute values these quantities are be more difficult to interpret than `'I_var'` and `'val_I_var'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot loss_train, the loss computed on training data as a function of epoch\n",
    "ax.plot(model.history['loss'],\n",
    "        label=r'loss_train')\n",
    "\n",
    "# Plot loss_val, the loss computed on validation data as a function of epoch\n",
    "ax.plot(model.history['val_loss'],\n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('Training hisotry: loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to consider more traditional metrics of model performance. In the context of GE models, a natural choice is $R^2$ between measurements $y$ and model predictions $\\hat{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "\n",
    "# Compute yhat on test data\n",
    "yhat_test = model.x_to_yhat(test_df['x'])\n",
    "\n",
    "# Compute R^2 between yhat_test and y_test\n",
    "Rsq = np.corrcoef(yhat_test.ravel(), test_df['y'])[0, 1]**2\n",
    "\n",
    "# Plot y_test vs. yhat_test\n",
    "ax.scatter(yhat_test, y_test, color='C0', s=10, alpha=.3,\n",
    "           label='test data')\n",
    "\n",
    "# Style plot\n",
    "xlim = [min(yhat_test), max(yhat_test)]\n",
    "ax.plot(xlim, xlim, '--', color='k', label='diagonal', zorder=100)\n",
    "ax.set_xlabel('model prediction ($\\hat{y}$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title(f'Standard metric of model performance:\\n$R^2$={Rsq:.3}');\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we visualize the GE measurement process inferred as part of our latent phenotype model. Recall from Tareen et al. (2021) that the measurement process consists of\n",
    "\n",
    "- A nonlinearity $\\hat{y} = g(\\phi)$ that deterministically maps the latent phenotype $\\phi$ to a prediction $\\hat{y}$.\n",
    "- A noise model $p(y|\\hat{y})$ that stochastically maps predictions $\\hat{y}$ to measurements $y$.\n",
    "\n",
    "We can conveniently visualize both of these quantities in a single \"global epistatsis plot\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "#dy_test = test_df['dy']\n",
    "\n",
    "# Compute Ï† on test data\n",
    "phi_test = model.x_to_phi(test_df['x'])\n",
    "\n",
    "## Set phi lims and create a grid in phi space\n",
    "phi_lim = [min(phi_test)-.5, max(phi_test)+.5]\n",
    "phi_grid = np.linspace(phi_lim[0], phi_lim[1], 1000)\n",
    "\n",
    "# Compute yhat each phi gridpoint\n",
    "yhat_grid = model.phi_to_yhat(phi_grid)\n",
    "\n",
    "# Compute 95% CI for each yhat\n",
    "q = [0.025, 0.975]\n",
    "yqs_grid = model.yhat_to_yq(yhat_grid, q=q)\n",
    "\n",
    "\n",
    "# Plot scatter of Ï† and y values.\n",
    "ix = np.random.choice(a=len(phi_test), size=5000, replace=False)\n",
    "ax.scatter(phi_test[ix], y_test[ix],\n",
    "           color='C0', s=10, alpha=.3, label='test data')\n",
    "\n",
    "# Plote 95% confidence interval\n",
    "ax.fill_between(phi_grid, yqs_grid[:, 0], yqs_grid[:, 1],\n",
    "                alpha=0.2, color='C1', lw=0, label='95% CI')\n",
    "\n",
    "# Plot GE nonlinearity\n",
    "ax.plot(phi_grid, yhat_grid,\n",
    "        linewidth=3, color='C1', label='nonlinearity')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlim(phi_lim)\n",
    "ax.set_xlabel('latent phenotype ($\\phi$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title('GE measurement process')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the values of our model's G-P map parameters, we use the mmethod `model.get_theta()`. This returns a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve G-P map parameter dict and view dict keys\n",
    "theta_dict = model.layer_gpmap.get_params()\n",
    "theta_dict.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to appreciate that G-P maps usually have many non-identifiable directions in parameter space. These are called \"gauge freedoms\".  Interpreting the values of model parameters requires that we first \"pin down\" these gauge freedoms by using a clearly specified convension. Specifying `gauge='consensus'` in `model.get_theta()` accomplishes this fixing all the $\\theta_{l:c}$ parameters that contribute to the consensus sequence to zero. This convension allows all the other $\\theta_{l:c}$ parameters in the additive model to be interpreted as single-mutation effects $\\Delta \\phi$ away from the consensus sequence.\n",
    "\n",
    "Finally, we use `mavenn.heatmap()` to visualize these additive parameters. This function takes a number of keyword arguments, which we summarize here. More information can be found in this function's docstring.\n",
    "\n",
    "- `ax=ax`: specifies the axes on which to draw both the heatmap and the colorbar.\n",
    "- `values=theta_dict['theta_lc']`: specifies the additive parameters in the form of a `np.array` of size `L`x`C`, where `C` is the alphabet size.\n",
    "- `alphabet=theta_dict['alphabet']`: provides a list of characters corresponding to the columns of `values`.\n",
    "- `seq=model.x_stats['consensus_seq']`: causes `mavenn.heatmap()` to highlight the characters of a specific sequence of interest. In our case this is the consensus sequence, the additive parameters for which are all fixed to zero.\n",
    "- `seq_kwargs={'c':'gray', 's':25}`: provides a keyword dictionary to pass to `ax.scatter()`; this specifies how the characters of the sequence of interest are to be graphically indicated.\n",
    "- `cmap='coolwarm'`: specifies the colormap used to represent the values of the additive parameters.\n",
    "- `cbar=True`: specifies that a colorbar be drawn\n",
    "- `cmap_size='2%'`: specifies the width of the colorbar relative to the enclosing ax object.\n",
    "- `cmap_pad=.3`: specifies the spacing between the heatmap and the colorbar\n",
    "- `ccenter=0`: centers the colormap at zero.\n",
    "\n",
    "This function returns two objects:\n",
    "- `heatmap_ax` is the axes object on which the heatmap is drawn.\n",
    "- `cb` is the colorbar object; it's corresponding axes is given by `cb.ax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mavenn.src.utils import _x_to_mat\n",
    "gb1_seq = model.x_stats['consensus_seq']\n",
    "x_lc_cons = _x_to_mat(gb1_seq, model.alphabet)\n",
    "ddG_b = theta_dict['theta_b_lc'] - np.sum(x_lc_cons*theta_dict['theta_b_lc'], axis=1)[:,np.newaxis]\n",
    "ddG_f = theta_dict['theta_f_lc'] - np.sum(x_lc_cons*theta_dict['theta_f_lc'], axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = plt.subplots(2,1, figsize=(12,10))\n",
    "\n",
    "# Draw heatmap\n",
    "heatmap_ax, cb = mavenn.heatmap(ax=axs[0],\n",
    "                                values=ddG_b,\n",
    "                                alphabet=model.alphabet,\n",
    "                                seq=model.x_stats['consensus_seq'],\n",
    "                                seq_kwargs={'c':'gray', 's':25},\n",
    "                                cmap='coolwarm',\n",
    "                                cbar=True,\n",
    "                                cmap_size='2%',\n",
    "                                cmap_pad=.3,\n",
    "                                ccenter=0)\n",
    "# Style heatmap (can be different between two dataset)\n",
    "#heatmap_ax.set_xticks()\n",
    "heatmap_ax.tick_params(axis='y', which='major', pad=10)\n",
    "heatmap_ax.set_xlabel('position ($l$)')\n",
    "heatmap_ax.set_ylabel('amino acid ($c$)')\n",
    "heatmap_ax.set_title(f'Binding energy matrix')\n",
    "\n",
    "# Style colorbar\n",
    "cb.outline.set_visible(False)\n",
    "cb.ax.tick_params(direction='in', size=20, color='white')\n",
    "cb.set_label('mutational effect ($\\Delta \\Delta G_F$)', labelpad=5, rotation=-90, ha='center', va='center')\n",
    "\n",
    "# Draw heatmap\n",
    "heatmap_ax, cb = mavenn.heatmap(ax=axs[1],\n",
    "                                values=ddG_f,\n",
    "                                alphabet=model.alphabet,\n",
    "                                seq=model.x_stats['consensus_seq'],\n",
    "                                seq_kwargs={'c':'gray', 's':25},\n",
    "                                cmap='coolwarm',\n",
    "                                cbar=True,\n",
    "                                cmap_size='2%',\n",
    "                                cmap_pad=.3,\n",
    "                                ccenter=0)\n",
    "# Style heatmap (can be different between two dataset)\n",
    "#heatmap_ax.set_xticks()\n",
    "heatmap_ax.tick_params(axis='y', which='major', pad=10)\n",
    "heatmap_ax.set_xlabel('position ($l$)')\n",
    "heatmap_ax.set_ylabel('amino acid ($c$)')\n",
    "heatmap_ax.set_title(f'Folding energy matrix')\n",
    "\n",
    "\n",
    "# Style colorbar\n",
    "cb.outline.set_visible(False)\n",
    "cb.ax.tick_params(direction='in', size=20, color='white')\n",
    "cb.set_label('mutational effect ($\\Delta \\Delta G_F$)', labelpad=5, rotation=-90, ha='center', va='center')\n",
    "\n",
    "# Adjust figure and show\n",
    "fig.tight_layout(w_pad=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many of the squares in the heatmap are white. These correspond to additive parameters whose values are `NaN`. MAVE-NN sets the values of a feature effect to `NaN` when no variant in the training set exhibits that feature. Such `NaN` parameters are common, even among additive parameters, as DMS libraries often do not contain a comprehensive set of single-amino-acid mutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Tareen, A., Posfai, A., Ireland, W. T., McCandlish, D. M. & Kinney, J. B. MAVE-NN: learning genotype-phenotype maps from multiplex assays of variant effect. bioRxiv doi:10.1101/2020.07.14.201475 (2020).\n",
    "1. Seuma, M., Faure, A., Badia, M., Lehner, B. & Bolognesi, B. The genetic landscape for amyloid beta fibril nucleation accurately discriminates familial Alzheimerâ€™s disease mutations. eLife 10, e63364 (2021).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}