{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tutorial 6: Using estimates of measurement uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Special imports\n",
    "import mavenn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we show how to train and visualize the `'amyloid_additive_ge'` and\n",
    "`'tdp43_additive_ge'` models, which are featured in Figs. 4 of Tareen et al.\n",
    "(2021). Both models are trained on datasets dataset provided with MAVE-NN\n",
    "(`'amyloid'` and `'tdp43'`, respectively).\n",
    "\n",
    "## Training\n",
    "\n",
    "First we choose which dataset we want to use and load it. We also compute the length of sequences in that dataset, as we will need this quantity for defining the architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'tdp43' \n",
      "Sequence length: 84 amino acids (+ stops)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>dist</th>\n",
       "      <th>y</th>\n",
       "      <th>dy</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.037438</td>\n",
       "      <td>NNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.009898</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>TNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>RNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030803</td>\n",
       "      <td>0.005341</td>\n",
       "      <td>SNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054716</td>\n",
       "      <td>0.035752</td>\n",
       "      <td>INSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57991</th>\n",
       "      <td>training</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.009706</td>\n",
       "      <td>0.035128</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57992</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>0.029436</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57993</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.086802</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57994</th>\n",
       "      <td>training</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.049587</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57995</th>\n",
       "      <td>training</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.105390</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57996 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              set  dist         y        dy  \\\n",
       "0        training     1  0.032210  0.037438   \n",
       "1        training     1 -0.009898  0.038981   \n",
       "2        training     1 -0.010471  0.005176   \n",
       "3        training     1  0.030803  0.005341   \n",
       "4        training     1 -0.054716  0.035752   \n",
       "...           ...   ...       ...       ...   \n",
       "57991    training     2 -0.009706  0.035128   \n",
       "57992  validation     2 -0.030744  0.029436   \n",
       "57993  validation     2 -0.086802  0.033174   \n",
       "57994    training     2 -0.049587  0.029130   \n",
       "57995    training     2 -0.105390  0.031189   \n",
       "\n",
       "                                                       x  \n",
       "0      NNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "1      TNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "2      RNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "3      SNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "4      INSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "...                                                  ...  \n",
       "57991  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "57992  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "57993  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "57994  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "57995  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "\n",
       "[57996 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "# data_name = 'amyloid'\n",
    "data_name = 'tdp43'\n",
    "print(f\"Loading dataset '{data_name}' \")\n",
    "\n",
    "# Load datset\n",
    "data_df = mavenn.load_example_dataset(data_name)\n",
    "\n",
    "# Get and report sequence length\n",
    "L = len(data_df.loc[0,'x'])\n",
    "print(f'Sequence length: {L:d} amino acids (+ stops)')\n",
    "\n",
    "# Preview dataset\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we split data into a training+validation set called `trainval_df`, and a\n",
    "test set called `test_df`, using the built-in function `mavenn.split_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   :   52,249 observations (  90.09%)\n",
      "Validation set :    2,916 observations (   5.03%)\n",
      "Test set       :    2,831 observations (   4.88%)\n",
      "-------------------------------------------------\n",
      "Total dataset  :   57,996 observations ( 100.00%)\n",
      "\n",
      "Train + val set size : 55,165 observations\n",
      "Test set size        :  2,831 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation</th>\n",
       "      <th>dist</th>\n",
       "      <th>y</th>\n",
       "      <th>dy</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.037438</td>\n",
       "      <td>NNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.009898</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>TNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>RNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030803</td>\n",
       "      <td>0.005341</td>\n",
       "      <td>SNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054716</td>\n",
       "      <td>0.035752</td>\n",
       "      <td>INSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55160</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.009706</td>\n",
       "      <td>0.035128</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55161</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>0.029436</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55162</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.086802</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55163</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.049587</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55164</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.105390</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55165 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       validation  dist         y        dy  \\\n",
       "0           False     1  0.032210  0.037438   \n",
       "1           False     1 -0.009898  0.038981   \n",
       "2           False     1 -0.010471  0.005176   \n",
       "3           False     1  0.030803  0.005341   \n",
       "4           False     1 -0.054716  0.035752   \n",
       "...           ...   ...       ...       ...   \n",
       "55160       False     2 -0.009706  0.035128   \n",
       "55161        True     2 -0.030744  0.029436   \n",
       "55162        True     2 -0.086802  0.033174   \n",
       "55163       False     2 -0.049587  0.029130   \n",
       "55164       False     2 -0.105390  0.031189   \n",
       "\n",
       "                                                       x  \n",
       "0      NNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "1      TNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "2      RNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "3      SNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "4      INSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "...                                                  ...  \n",
       "55160  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "55161  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "55162  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "55163  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "55164  GNSRGGGAGLGNNQGSNMGGGMNFGAFSINPAMMAAAQAALQSSWG...  \n",
       "\n",
       "[55165 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset\n",
    "trainval_df, test_df = mavenn.split_dataset(data_df)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f'Train + val set size : {len(trainval_df):6,d} observations')\n",
    "print(f'Test set size        : {len(test_df):6,d} observations')\n",
    "\n",
    "# Preview trainval_df\n",
    "trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we specify the architecture of our model. To do this, we create an instance of the `mavenn.Model` class, called `model`, using the following keyword arguments:\n",
    "\n",
    "- `L=L` specifies sequence length.\n",
    "- `alphabet='protein*'` specifies that the alphabet our sequences are built from consists of 21 characters representing the 20 amino acids plus a stop signal (which is represented by the character `'*'`).\n",
    "- `gpmap_type='additive'` specifies that we wish to infer an additive G-P map.\n",
    "- `regression_type='GE'` specifies that our model will have a global epistasis (GE) measurement process. We choose this because our MAVE measurements are continuous real numbers.\n",
    "- `ge_noise_model_type='SkewedT'` specifies the use of a skewed-t noise model in the GE measurement process. The `'SkewedT'` noise model can accommodate asymmetric noise and is thus more flexible than the default `'Gaussian'` noise model.\n",
    "- `ge_heteroskedasticity_order=2` specifies that the noise model parameters (the three parameters of the skewed-t distribution) are each modeled using quadratic functions of the predicted measurement $\\hat{y}$. This will allow our model of experimental noise to vary with signal intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We then set the training data by calling `model.set_data()`. The keyword argument `'validation_flags'` is used to specify which subset of the data in `trainval_df` will be used for validation (as opposed to stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we train the model by calling `model.fit()`. In doing so we specify a number of hyperparameters including the learning rate, the number of epochs, the batch size, whether to use early stopping, and the early stopping patience.  We also set `verbose=False` to limit the amount of user feedback.\n",
    "\n",
    "Choosing hyperparameters is somewhat of an art, and the particular values used here were found by trial and error. In general users will have to try a number of different values for these and possibly other hyperparameters in order to find ones that work well. We recommend that users choose these hyperparameters in order to maximize the final value for `val_I_var`, the variational information of the trained model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 55,165 observations set as training data.\n",
      "Using 5.3% for validation.\n",
      "Data shuffled.\n",
      "Time to set data: 1.5 sec.\n",
      " \n",
      "LSMR            Least-squares solution of  Ax = b\n",
      "\n",
      "The matrix A has 52249 rows and 1764 columns\n",
      "damp = 0.00000000000000e+00\n",
      "\n",
      "atol = 1.00e-06                 conlim = 1.00e+08\n",
      "\n",
      "btol = 1.00e-06             maxiter =     1764\n",
      "\n",
      " \n",
      "   itn      x(1)       norm r    norm Ar  compatible   LS      norm A   cond A\n",
      "     0  0.00000e+00  2.287e+02  4.621e+03   1.0e+00  8.8e-02\n",
      "     1 -1.43312e-07  2.286e+02  4.612e+03   1.0e+00  1.6e-01  1.3e+02  1.0e+00\n",
      "     2 -3.07641e-04  1.830e+02  2.515e+03   8.0e-01  6.7e-03  2.0e+03  5.9e+01\n",
      "     3 -2.07725e-02  1.630e+02  2.049e+03   7.1e-01  6.1e-03  2.0e+03  8.1e+01\n",
      "     4 -9.83907e-02  9.032e+01  7.998e+02   3.9e-01  4.3e-03  2.0e+03  1.1e+02\n",
      "     5 -1.13771e-01  8.455e+01  3.831e+02   3.7e-01  2.2e-03  2.0e+03  1.0e+02\n",
      "     6 -1.16596e-01  8.426e+01  2.370e+02   3.7e-01  1.1e-03  2.6e+03  1.0e+02\n",
      "     7 -1.16549e-01  8.368e+01  1.749e+02   3.7e-01  7.2e-04  2.9e+03  1.0e+02\n",
      "     8 -1.13500e-01  8.268e+01  1.021e+02   3.6e-01  4.3e-04  2.9e+03  1.4e+02\n",
      "     9 -1.12784e-01  8.249e+01  7.246e+01   3.6e-01  3.0e-04  2.9e+03  1.1e+02\n",
      "    10 -1.12783e-01  8.249e+01  7.246e+01   3.6e-01  3.0e-04  2.9e+03  2.5e+02\n",
      "    20 -6.46526e-02  7.845e+01  3.430e+01   3.4e-01  9.6e-05  4.6e+03  1.8e+02\n",
      "    30 -6.53674e-02  7.705e+01  1.972e+01   3.4e-01  4.7e-05  5.4e+03  5.3e+02\n",
      "    40 -9.11242e-02  7.624e+01  9.974e+00   3.3e-01  2.1e-05  6.1e+03  4.0e+02\n",
      "    50 -1.04652e-01  7.600e+01  5.957e+00   3.3e-01  1.2e-05  6.8e+03  2.1e+02\n",
      "    60 -1.13486e-01  7.591e+01  3.733e+00   3.3e-01  6.4e-06  7.7e+03  2.3e+02\n",
      "    70 -1.20253e-01  7.586e+01  2.016e+00   3.3e-01  3.2e-06  8.2e+03  3.3e+02\n",
      "    80 -1.23501e-01  7.585e+01  1.053e+00   3.3e-01  1.6e-06  8.7e+03  2.3e+02\n",
      "    87 -1.23983e-01  7.585e+01  5.984e-01   3.3e-01  8.6e-07  9.2e+03  2.3e+02\n",
      " \n",
      "LSMR finished\n",
      "The least-squares solution is good enough, given atol     \n",
      "istop =       2    normr = 7.6e+01\n",
      "    normA = 9.2e+03    normAr = 6.0e-01\n",
      "itn   =      87    condA = 2.3e+02\n",
      "    normx = 2.5e+01\n",
      "    87 -1.23983e-01   7.585e+01  5.984e-01\n",
      "   3.3e-01  8.6e-07   9.2e+03  2.3e+02\n",
      "Linear regression time: 1.9279 sec\n",
      "Epoch 1/500\n",
      "\u001b[1m780/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7047.0586"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmpiricalGaussianNoiseModelLayer.compute_nlls() missing 1 required positional argument: 'dy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mset_data(x\u001b[38;5;241m=\u001b[39mtrainval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m                y\u001b[38;5;241m=\u001b[39mtrainval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m                dy\u001b[38;5;241m=\u001b[39mtrainval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m                validation_flags\u001b[38;5;241m=\u001b[39mtrainval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/error_handling.py:98\u001b[0m, in \u001b[0;36mhandle_errors.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m mistake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Execute function\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# If running functional test and expect to fail\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_fail \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/model.py:856\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, epochs, learning_rate, validation_split, verbose, early_stopping, early_stopping_patience, batch_size, linear_initialization, freeze_theta, callbacks, try_tqdm, optimizer, optimizer_kwargs, fit_kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# Train neural network using TensorFlow\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                               \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# # Get function representing the raw gp_map\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# self._unfixed_gpmap = K.function(\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m#     [self.model.model.layers[1].input],\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m#     [self.model.model.layers[2].output])\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# Replace the K.function() call with this:\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_unfixed_gpmap\u001b[39m(inputs):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/callbacks.py:61\u001b[0m, in \u001b[0;36mIVariationalCallback.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate I_variational and store in logs dict\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     I_var, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmavenn_model\u001b[38;5;241m.\u001b[39mI_variational(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, uncertainty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m     logs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_name] \u001b[38;5;241m=\u001b[39m I_var\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/error_handling.py:98\u001b[0m, in \u001b[0;36mhandle_errors.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m mistake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Execute function\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# If running functional test and expect to fail\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_fail \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/model.py:1539\u001b[0m, in \u001b[0;36mModel.I_variational\u001b[0;34m(self, x, y, ct, knn_fuzz, uncertainty)\u001b[0m\n\u001b[1;32m   1536\u001b[0m phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_to_phi(x)\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;66;03m# Compute p_y_give_phi\u001b[39;00m\n\u001b[0;32m-> 1539\u001b[0m p_y_given_phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_of_y_given_phi\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mpaired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Compute H_y_given_phi\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m H_y_given_phi_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog2(p_y_given_phi \u001b[38;5;241m+\u001b[39m TINY)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/model.py:1868\u001b[0m, in \u001b[0;36mModel.p_of_y_given_phi\u001b[0;34m(self, y, phi, paired)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_to_yhat(phi)\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# Comptue p_y_given_phi using yhat\u001b[39;00m\n\u001b[0;32m-> 1868\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_of_y_given_yhat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;66;03m# Otherwise, just compute p\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1872\u001b[0m \n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;66;03m# Cast y as integers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/model.py:1972\u001b[0m, in \u001b[0;36mModel.p_of_y_given_yhat\u001b[0;34m(self, y, yhat, paired)\u001b[0m\n\u001b[1;32m   1969\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_noise_model\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;66;03m# Compute p_norm using layer\u001b[39;00m\n\u001b[0;32m-> 1972\u001b[0m p_norm \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_of_y_given_yhat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myhat_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_arrays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;66;03m# Unnormalize p\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m p \u001b[38;5;241m=\u001b[39m p_norm \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_std\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/layers/measurement_process_layers.py:94\u001b[0m, in \u001b[0;36mhandle_arrays.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         kwargs[key] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(arg, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Pass inputs to func and get outputs\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Convert results within a tuple\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/layers/measurement_process_layers.py:479\u001b[0m, in \u001b[0;36mNoiseModelLayer.p_of_y_given_yhat\u001b[0;34m(self, y, yhat)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute p(y|yhat).\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Compute negative log likeliihood\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m nll_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_nlls\u001b[49m\u001b[43m(\u001b[49m\u001b[43myhat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myhat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mytrue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# Convert to p_y_given_yhat and return\u001b[39;00m\n\u001b[1;32m    483\u001b[0m p_y_given_yhat \u001b[38;5;241m=\u001b[39m Exp(\u001b[38;5;241m-\u001b[39mnll_arr)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/test_mavenn/lib/python3.12/site-packages/mavenn/src/layers/measurement_process_layers.py:108\u001b[0m, in \u001b[0;36mhandle_arrays.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, tf\u001b[38;5;241m.\u001b[39mTensor) \\\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;28;01melse\u001b[39;00m result\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Otherwise, just use raw function.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Retun (potentially) adjusted results\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: EmpiricalGaussianNoiseModelLayer.compute_nlls() missing 1 required positional argument: 'dy'"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = mavenn.Model(L=L,\n",
    "                     alphabet='protein*',\n",
    "                     gpmap_type='additive',\n",
    "                     regression_type='GE',\n",
    "                     ge_noise_model_type='Empirical')\n",
    "\n",
    "# Set training data\n",
    "model.set_data(x=trainval_df['x'],\n",
    "               y=trainval_df['y'],\n",
    "               dy=trainval_df['dy'],\n",
    "               validation_flags=trainval_df['validation'])\n",
    "\n",
    "# Train model\n",
    "model.fit(learning_rate=1e-3,\n",
    "          epochs=500,\n",
    "          batch_size=64,\n",
    "          early_stopping=True,\n",
    "          early_stopping_patience=25,\n",
    "          verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To assess the performance of our final trained model, we compute two different metrics on test data: **variational information** and **predictive information**. Variational information quantifies the performance of the full latent phenotype model, whereas predictive information quantifies the performance of just the G-P map portion of the model. See Tareen et al. (2021) for an expanded discussion of these quantities.\n",
    "\n",
    "Note that MAVE-NN also estimates the standard errors for these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predictive information on test data\n",
    "I_pred, dI_pred = model.I_predictive(x=test_df['x'], y=test_df['y'])\n",
    "print(f'test_I_pred: {I_pred:.3f} +- {dI_pred:.3f} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To save the trained model we call `model.save()`. This records our model in **two separate files**: a pickle file that defines model architecture (extension `'.pickle'`), and an H5 file that records model parameters (extension `'.h5'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "model_name = f'{data_name}_additive_ge_empirical'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualization\n",
    "\n",
    "We now discuss how to visualize the training history, performance, and parameters of a trained model. First we  then load our model using `mavenn.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Delete model if it is present in memory\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load model from file\n",
    "model = mavenn.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `model.history` dictionary contains values for multiple model performance metrics, evaluated on both the training data and the validation data, as a function of training epoch. `'loss'` and `'val_loss'` record loss values, while `'I_var'` and `'val_I_var'` record the variational information values. As described in Tareen et al. (2021), these metrics are closely related: variational information is an affine transformation of model likelihood, whereas loss is equal to likelihood plus regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show metrics recorded in model.history()\n",
    "model.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plotting `'I_var'` and `'val_I_var'` versus epoch can often provide insight into the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot I_var_train, the variational information on training data as a function of epoch\n",
    "ax.plot(model.history['I_var'],\n",
    "        label=r'I_var_train')\n",
    "\n",
    "# Plot I_var_val, the variational information on validation data as a function of epoch\n",
    "ax.plot(model.history['val_I_var'],\n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Show I_pred_test, the predictive information of the final model on test data\n",
    "ax.axhline(I_pred, color='C3', linestyle=':',\n",
    "           label=r'test_I_pred')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('bits')\n",
    "ax.set_title('Training history: variational information')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From this plot we observe a few things:\n",
    "\n",
    "- `'val_I_var'` quickly converges to its final value, suggesting that model training has likely gone to completion.\n",
    "- `'I_var'` is noticably higher than `'val_I_var'`, which is indicative of overfitting. But these two quantities largely remain monotonically related to one another, indicating that this overfitting is likely begnin.\n",
    "- `'test_I_var'` is quite close to `'test_I_pred'` indicating that the inferred measurement process does a good job of describing the scatter of $y$ about $\\hat{y}$.\n",
    "\n",
    "Users can plot '`loss`' and '`var_loss`' if they like, though the absolute values these quantities are be more difficult to interpret than `'I_var'` and `'val_I_var'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot loss_train, the loss computed on training data as a function of epoch\n",
    "ax.plot(model.history['loss'],\n",
    "        label=r'loss_train')\n",
    "\n",
    "# Plot loss_val, the loss computed on validation data as a function of epoch\n",
    "ax.plot(model.history['val_loss'],\n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('Training hisotry: loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is also useful to consider more traditional metrics of model performance. In the context of GE models, a natural choice is $R^2$ between measurements $y$ and model predictions $\\hat{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "\n",
    "# Compute yhat on test data\n",
    "yhat_test = model.x_to_yhat(test_df['x'])\n",
    "\n",
    "# Compute R^2 between yhat_test and y_test\n",
    "Rsq = np.corrcoef(yhat_test.ravel(), test_df['y'])[0, 1]**2\n",
    "\n",
    "# Plot y_test vs. yhat_test\n",
    "ax.scatter(yhat_test, y_test, color='C0', s=10, alpha=.3,\n",
    "           label='test data')\n",
    "\n",
    "# Style plot\n",
    "xlim = [min(yhat_test), max(yhat_test)]\n",
    "ax.plot(xlim, xlim, '--', color='k', label='diagonal', zorder=100)\n",
    "ax.set_xlabel('model prediction ($\\hat{y}$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title(f'Standard metric of model performance:\\n$R^2$={Rsq:.3}');\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we visualize the GE measurement process inferred as part of our latent phenotype model. Recall from Tareen et al. (2021) that the measurement process consists of\n",
    "\n",
    "- A nonlinearity $\\hat{y} = g(\\phi)$ that deterministically maps the latent phenotype $\\phi$ to a prediction $\\hat{y}$.\n",
    "- A noise model $p(y|\\hat{y})$ that stochastically maps predictions $\\hat{y}$ to measurements $y$.\n",
    "\n",
    "We can conveniently visualize both of these quantities in a single \"global epistatsis plot\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "dy_test = test_df['dy']\n",
    "\n",
    "# Compute φ on test data\n",
    "phi_test = model.x_to_phi(test_df['x'])\n",
    "\n",
    "## Set phi lims and create a grid in phi space\n",
    "phi_lim = [min(phi_test)-.5, max(phi_test)+.5]\n",
    "phi_grid = np.linspace(phi_lim[0], phi_lim[1], 1000)\n",
    "\n",
    "# Compute yhat each phi gridpoint\n",
    "yhat_grid = model.phi_to_yhat(phi_grid)\n",
    "\n",
    "# Compute 95% CI for each yhat\n",
    "#q = [0.025, 0.975]\n",
    "#yqs_grid = model.yhat_to_yq(yhat_grid, q=q, dy=dy_test)\n",
    "\n",
    "\n",
    "# Plote 95% confidence interval\n",
    "# ax.fill_between(phi_grid, yqs_grid[:, 0], yqs_grid[:, 1],\n",
    "#                 alpha=0.2, color='C1', lw=0, label='95% CI')\n",
    "\n",
    "# Plot GE nonlinearity\n",
    "ax.plot(phi_grid, yhat_grid,\n",
    "        linewidth=3, color='C1', label='nonlinearity')\n",
    "\n",
    "# Plot scatter of φ and y values.\n",
    "ax.scatter(phi_test, y_test,\n",
    "           color='C0', s=10, alpha=.3, label='test data', zorder=+100)\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlim(phi_lim)\n",
    "ax.set_xlabel('latent phenotype ($\\phi$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title('GE measurement process')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To retrieve the values of our model's G-P map parameters, we use the mmethod `model.get_theta()`. This returns a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve G-P map parameter dict and view dict keys\n",
    "theta_dict = model.get_theta(gauge='consensus')\n",
    "theta_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is important to appreciate that G-P maps usually have many non-identifiable directions in parameter space. These are called \"gauge freedoms\".  Interpreting the values of model parameters requires that we first \"pin down\" these gauge freedoms by using a clearly specified convension. Specifying `gauge='consensus'` in `model.get_theta()` accomplishes this fixing all the $\\theta_{l:c}$ parameters that contribute to the consensus sequence to zero. This convension allows all the other $\\theta_{l:c}$ parameters in the additive model to be interpreted as single-mutation effects $\\Delta \\phi$ away from the consensus sequence.\n",
    "\n",
    "Finally, we use `mavenn.heatmap()` to visualize these additive parameters. This function takes a number of keyword arguments, which we summarize here. More information can be found in this function's docstring.\n",
    "\n",
    "- `ax=ax`: specifies the axes on which to draw both the heatmap and the colorbar.\n",
    "- `values=theta_dict['theta_lc']`: specifies the additive parameters in the form of a `np.array` of size `L`x`C`, where `C` is the alphabet size.\n",
    "- `alphabet=theta_dict['alphabet']`: provides a list of characters corresponding to the columns of `values`.\n",
    "- `seq=model.x_stats['consensus_seq']`: causes `mavenn.heatmap()` to highlight the characters of a specific sequence of interest. In our case this is the consensus sequence, the additive parameters for which are all fixed to zero.\n",
    "- `seq_kwargs={'c':'gray', 's':25}`: provides a keyword dictionary to pass to `ax.scatter()`; this specifies how the characters of the sequence of interest are to be graphically indicated.\n",
    "- `cmap='coolwarm'`: specifies the colormap used to represent the values of the additive parameters.\n",
    "- `cbar=True`: specifies that a colorbar be drawn\n",
    "- `cmap_size='2%'`: specifies the width of the colorbar relative to the enclosing ax object.\n",
    "- `cmap_pad=.3`: specifies the spacing between the heatmap and the colorbar\n",
    "- `ccenter=0`: centers the colormap at zero.\n",
    "\n",
    "This function returns two objects:\n",
    "- `heatmap_ax` is the axes object on which the heatmap is drawn.\n",
    "- `cb` is the colorbar object; it's corresponding axes is given by `cb.ax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "# Draw heatmap\n",
    "heatmap_ax, cb = mavenn.heatmap(ax=ax,\n",
    "                                values=theta_dict['theta_lc'],\n",
    "                                alphabet=theta_dict['alphabet'],\n",
    "                                seq=model.x_stats['consensus_seq'],\n",
    "                                seq_kwargs={'c':'gray', 's':25},\n",
    "                                cmap='coolwarm',\n",
    "                                cbar=True,\n",
    "                                cmap_size='2%',\n",
    "                                cmap_pad=.3,\n",
    "                                ccenter=0)\n",
    "# Style heatmap (can be different between two dataset)\n",
    "#heatmap_ax.set_xticks()\n",
    "heatmap_ax.tick_params(axis='y', which='major', pad=10)\n",
    "heatmap_ax.set_xlabel('position ($l$)')\n",
    "heatmap_ax.set_ylabel('amino acid ($c$)')\n",
    "heatmap_ax.set_title(f'Additive parameters: {model_name}')\n",
    "\n",
    "# Style colorbar\n",
    "cb.outline.set_visible(False)\n",
    "cb.ax.tick_params(direction='in', size=20, color='white')\n",
    "cb.set_label('mutational effect ($\\Delta \\phi$)', labelpad=5, rotation=-90, ha='center', va='center')\n",
    "\n",
    "# Adjust figure and show\n",
    "fig.tight_layout(w_pad=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that many of the squares in the heatmap are white. These correspond to additive parameters whose values are `NaN`. MAVE-NN sets the values of a feature effect to `NaN` when no variant in the training set exhibits that feature. Such `NaN` parameters are common, even among additive parameters, as DMS libraries often do not contain a comprehensive set of single-amino-acid mutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## References\n",
    "\n",
    "1. Tareen, A., Posfai, A., Ireland, W. T., McCandlish, D. M. & Kinney, J. B. MAVE-NN: learning genotype-phenotype maps from multiplex assays of variant effect. bioRxiv doi:10.1101/2020.07.14.201475 (2020).\n",
    "1. Seuma, M., Faure, A., Badia, M., Lehner, B. & Bolognesi, B. The genetic landscape for amyloid beta fibril nucleation accurately discriminates familial Alzheimer’s disease mutations. eLife 10, e63364 (2021).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
